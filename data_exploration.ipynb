{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bourbon-Rye/Baesian-Cropability/blob/main/PilipiNuts_2023_Baesian_Cropability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# @Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import re\n",
        "import functools\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn import preprocessing\n",
        "from plotly.subplots import make_subplots\n",
        "from scipy.stats import chi2_contingency\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# @Utilities\n",
        "def read_csv_to_df(csvfile: Path, comment_symbol='#') -> pd.DataFrame:\n",
        "    \"\"\"Read CSV to DataFrame with comment validation.\n",
        "        Allows comment lines in CSVs where line[0] == comment_symbol.\n",
        "        Also removes newlines.\n",
        "        Does not recognize comment_symbol anywhere else. \n",
        "    \"\"\"\n",
        "    tempfile = Path('temp.csv')\n",
        "    with open(csvfile, 'r') as csv, open(tempfile, 'w+') as temp:\n",
        "        lines = csv.readlines()\n",
        "        for line in lines:\n",
        "            if line[0] != comment_symbol:\n",
        "                temp.write(line)\n",
        "    return pd.read_csv(tempfile)\n",
        "\n",
        "with open('datasets/region_provinces.json') as jsonfile:\n",
        "    regions_provinces = json.load(jsonfile)['PHILIPPINES']\n",
        "regions_provinces = {key.lower():regions_provinces[key] for key in regions_provinces}\n",
        "regions = regions_provinces.keys()\n",
        "provinces = set([item.lower() for key in regions for item in regions_provinces[key]])\n",
        "regions = set(regions)\n",
        "\n",
        "# NOTE: Fix for bad regions, thanks PSA\n",
        "bad_regions = ['AONCR', 'BARMM', 'CAR', 'MIMAROPA', 'NCR',\n",
        "           'Region 1', 'Region 2', 'Region 3', 'Region 4A',\n",
        "           'Region 5', 'Region 6', 'Region 7', 'Region 8',\n",
        "           'Region 9', 'Region 10', 'Region 11', 'Reg12', 'CARAGA']\n",
        "region_mapping = {bad.lower():good.lower() for (bad,good) in zip(bad_regions, regions_provinces.keys())}\n",
        "corrections = {\n",
        "    \"AUTONOMOUS REGION IN MUSLIM MINDANAO (ARMM)\": \"bangsamoro autonomous region in muslim mindanao (barmm)\",\n",
        "    \"autonomous region in muslim mindanao (armm)\": \"bangsamoro autonomous region in muslim mindanao (barmm)\",\n",
        "    \"mimaropa region\": \"mimaropa region (mimaropa)\"\n",
        "}\n",
        "region_mapping.update(corrections)\n",
        "\n",
        "temp = [key.split('(')[1].rstrip(')') for key in regions_provinces]\n",
        "region_short_to_short_long = {bad:good for (bad,good) in zip(temp, regions_provinces.keys())}\n",
        "print(sorted(regions))\n",
        "print(region_short_to_short_long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_region(x: str, regions=regions) -> bool:\n",
        "    x = x.strip(' .')\n",
        "    x = x.lower()\n",
        "    if x == \"cagayan\":\n",
        "        return False\n",
        "    for region in regions:\n",
        "        if x in region:\n",
        "            return True\n",
        "    else:\n",
        "        return False\n",
        "    \n",
        "def is_province(x: str, provinces=provinces) -> bool:\n",
        "    x = x.strip(' .')\n",
        "    x = x.lower()\n",
        "    for province in provinces:\n",
        "        if x in province:\n",
        "            return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def get_quarter_columns(df: pd.DataFrame, year_range: range, period_idx: int):\n",
        "    \"\"\"Assumes contiguous period columns and that columns before  are ID columns\"\"\"\n",
        "    df_quarter = df.iloc[:, :period_idx]\n",
        "    for year in year_range:\n",
        "        for q in range(0, 12, 3):\n",
        "            df_quarter[f\"{year} Q{q//3+1}\"] = df.filter(regex=str(year), axis=1).iloc[:, q:q+3].mean(axis=1)\n",
        "    return df_quarter\n",
        "\n",
        "def get_annual_columns(df: pd.DataFrame, year_range: range, period_idx: int):\n",
        "    \"\"\"Assumes contiguous period columns and that columns before year_start are ID columns\"\"\"\n",
        "    df_annual = df.iloc[:, :period_idx]\n",
        "    for year in year_range:\n",
        "        df_annual[f\"{year}\"] = df.filter(regex=str(year), axis=1).mean(axis=1)\n",
        "    return df_annual\n",
        "\n",
        "def swap_columns(df: pd.DataFrame, col1: str, col2: str):\n",
        "    col_list = list(df.columns)\n",
        "    x, y = col_list.index(col1), col_list.index(col2)\n",
        "    col_list[y], col_list[x] = col_list[x], col_list[y]\n",
        "    df = df[col_list]\n",
        "    return df\n",
        "\n",
        "def normalize(df: pd.DataFrame, col: str, minmax = True):\n",
        "    \"\"\"Can use mean normalization and minmax normalization.\"\"\"\n",
        "    tdf = df[col]\n",
        "    if minmax:\n",
        "        tdf = (tdf-tdf.min())/(tdf.max()-tdf.min())\n",
        "    else:\n",
        "        tdf = (tdf-tdf.mean())/tdf.std()\n",
        "    df[col] = tdf\n",
        "    return df\n",
        "\n",
        "def drop_rows_with_zeros(df: pd.DataFrame, ref_col_idx: int, all_zeros=False):\n",
        "    \"\"\"Drop rows if some values are zeros, or if all values are zeros.\n",
        "    Assumes contiguous reference columns, i.e. columns to use in deciding whether to drop.\"\"\"\n",
        "    return df[~(df.iloc[:, ref_col_idx:] == 0).all(axis=1)] if all_zeros else df[~(df.iloc[:, ref_col_idx:] == 0).any(axis=1)]\n",
        "\n",
        "def dual_plot(df: pd.DataFrame, y1: str, y2: str,\n",
        "              x_title: str, y1_title: str, y2_title: str, title_text: str):\n",
        "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "    # Add traces\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=df.index, y=df[y1], name=y1),\n",
        "        secondary_y=False,\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=df.index, y=df[y2], name=y2),\n",
        "        secondary_y=True,\n",
        "    )\n",
        "    # Add titles\n",
        "    fig.update_layout(title_text=title_text)\n",
        "    fig.update_xaxes(title_text=x_title)\n",
        "    # Set y-axes titles\n",
        "    fig.update_yaxes(title_text=y1_title, secondary_y=False)\n",
        "    fig.update_yaxes(title_text=y2_title, secondary_y=True)\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "def show_heatmap(df: pd.DataFrame):\n",
        "    fig = plt.figure(figsize=(15,10))\n",
        "    sns.heatmap(df.corr(method='pearson'), annot = True, cmap=\"Blues\")\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    plt.show()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OTG Cleanup\n",
        "# VALUE OF PRODUCTION (5/24/2024)\n",
        "datapath = Path(\"datasets/agricultural-indicators/value-of-production/\")\n",
        "writepath = Path(\"datasets\")\n",
        "\n",
        "files = list(datapath.glob(\"*.csv\"))\n",
        "df = pd.DataFrame()\n",
        "for file in files:\n",
        "    with open(file) as f:\n",
        "        region = f.readlines()[0].split(':')[0].lstrip(\"\\\"\")\n",
        "        if region == u\"\\ufeffTest\\n\":\n",
        "            region = \"Cordillera Administrative Region (CAR)\"\n",
        "        tdf = pd.read_csv(file, skiprows=1, na_values=\".\")\n",
        "        tdf[\"Geolocation\"] = region\n",
        "        df = pd.concat([df, tdf])\n",
        "df[\"Subsector\"] = \"Rice\"\n",
        "df.drop(columns=[\"Type of Valuation\"], inplace=True)\n",
        "df.rename({\"Subsector\": \"Commodity\"}, axis=1, inplace=True)\n",
        "\n",
        "# Imputation of VOP df\n",
        "imp = SimpleImputer(missing_values=pd.NA, strategy='mean')\n",
        "imp.fit(df.iloc[:, 1:-1])\n",
        "df[df.columns[1:-1]] = imp.transform(df.iloc[:, 1:-1])\n",
        "df_val_of_prod = df\n",
        "\n",
        "# Annualized rice stocks NOTE: Perhaps try to match this with the prices as this has monthly, for national only\n",
        "df = read_csv_to_df(\"datasets/cropyield/stocks-palay-corn_yearly_1980-2024.csv\")\n",
        "df.dropna(inplace=True)\n",
        "df = drop_rows_with_zeros(df, ref_col_idx=2, all_zeros=True)\n",
        "df = df[df[\"Sector\"] == \"Rice: Total Stock\"]\n",
        "df[\"Sector\"] = \"Rice\"\n",
        "df = pd.concat([df.iloc[:, :2], df.mean(axis=1, numeric_only=True)], axis=1)\n",
        "df.rename(columns={\"Sector\": \"Commodity\", \"Year\": \"Period\", 0: \"Stocks\"}, inplace=True)\n",
        "df[\"Period\"] = df[\"Period\"].astype(object)\n",
        "df = df[df[\"Period\"].isin(range(2012, 2024))]\n",
        "df_stocks = df.set_index(\"Period\", drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv0jREFhuh32"
      },
      "source": [
        "# Baesian Plots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JToPchIbvMj_"
      },
      "source": [
        "## Yenzy Plots\n",
        "H0.2: There is no significant relationship between market conditions and food crop production.\n",
        "\n",
        "Goal: Visually and statistically assess the relationship between market indicators and crop yield.\n",
        "\n",
        "Visual tests are dual plots and scatterplots. Statistics for relationship testing: contingency table and chi-square test.\n",
        "\n",
        "**Tested Assessments in both Annual & Quarterly and Regional & Provincial**\n",
        "| Cropyield Indicator | Market Indicator | Geolocation | Commodity\n",
        "| --- | --- | --- | --- |\n",
        "| Volume | Retail Price | Philippines | Rice | \n",
        "| Volume | Wholesale Price | Philippines | Rice | \n",
        "| Volume | Farmgate Price | Philippines | Rice | "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample Relationship Test\n",
        "data = [[207, 282, 241], [234, 242, 232]]\n",
        "stat, p, dof, expected = chi2_contingency(data)\n",
        " \n",
        "# interpret p-value\n",
        "alpha = 0.05\n",
        "print(\"p value is \" + str(p))\n",
        "if p <= alpha:\n",
        "    print('Dependent (reject H0)')\n",
        "else:\n",
        "    print('Independent (H0 holds true)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Volume of Production of Palay and Corn across the Years (Quarterly and Annual)\n",
        "df2 = read_csv_to_df(\"datasets/agricultural-indicators/Volume_Rice and Corn_quarterly.csv\")\n",
        "df2 = df2[df2[\"Geolocation\"] == \"PHILIPPINES\"]\n",
        "df2 = df2.melt(id_vars=[\"Commodity\", \"Geolocation\"], value_vars=df2.columns[2:], var_name=\"Period\", value_name=\"Volume\")\n",
        "fig = px.line(df2, x=\"Period\", y=\"Volume\", color=\"Commodity\", title='Quarterly Volume of Production of Crops across the Years<br>National Averages in Metric Tons').update_layout(\n",
        "    xaxis_title=\"Period\", yaxis_title = \"Volume of Production\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating megadataset for rice\n",
        "def preprocess_baesians_1(df: pd.DataFrame, commodity: str, rename_to: str, melt_value: str|None, regional=True,\n",
        "                          impute=False):\n",
        "    \"\"\"Assumes Geolocation | Commodity | Period columns.\n",
        "    Filters to regional if regional=True, else filters to provincial.\n",
        "    \"\"\"\n",
        "    # df.dropna(inplace=True)\n",
        "    df[\"Geolocation\"] = df[\"Geolocation\"].str.lstrip(\".\").str.lower()\n",
        "    df[\"Geolocation\"] = df[\"Geolocation\"].replace(region_mapping)\n",
        "    df = df[df[\"Geolocation\"].apply(is_region)] if regional else df[df[\"Geolocation\"].apply(is_province)]\n",
        "    df = df[df[\"Commodity\"] == commodity]\n",
        "    df[\"Commodity\"] = rename_to # Rename everything in Commodity to Rice\n",
        "    if melt_value:\n",
        "        df = df.melt(id_vars=[\"Geolocation\", \"Commodity\"], value_vars=df.columns[2:], var_name=\"Period\", value_name=melt_value)\n",
        "    return df\n",
        "\n",
        "# RICE: Volume and Retail Price (Quarterly-Regional)\n",
        "# Volume of Rice\n",
        "df1 = read_csv_to_df(\"datasets/agricultural-indicators/Volume_Rice and Corn_quarterly.csv\")\n",
        "df1 = preprocess_baesians_1(df1, \"Palay\", \"Rice\", melt_value=\"Volume\")\n",
        "\n",
        "# Retail Price of Rice\n",
        "df2 = read_csv_to_df(\"datasets/prices/prices_retail_2012-2023.csv\")\n",
        "tdf1 = pd.read_csv(\"datasets/prices/prices_retail_caraga_supplement.csv\", skiprows=1)\n",
        "tdf2 = pd.read_csv(\"datasets/prices/prices_retail_car_supplement.csv\", skiprows=1)\n",
        "df2 = pd.concat([df2, tdf1, tdf2])\n",
        "df2 = get_quarter_columns(df2, range(2012, 2024), 2)\n",
        "df2 = preprocess_baesians_1(df2, \"RICE, REGULAR-MILLED, 1 KG\", \"Rice\", melt_value=\"Retail Price\")\n",
        "\n",
        "# Wholesale Price of Rice\n",
        "df3 = pd.read_csv(\"datasets/prices/prices_wholesale-new-series_2010-2023.csv\",)\n",
        "df3 = get_quarter_columns(df3, range(2012, 2024), 2)\n",
        "df3 = preprocess_baesians_1(df3, \"Well Milled Rice (WMR)\", \"Rice\", melt_value=\"Wholesale Price\")\n",
        "\n",
        "# Farmgate Price of Rice\n",
        "df4 = read_csv_to_df(\"datasets/prices/prices_farmgate-new-series_2010-2023.csv\")\n",
        "df4 = get_quarter_columns(df4, range(2012, 2024), 2)\n",
        "df4 = preprocess_baesians_1(df4, \"Palay [Paddy] Other Variety, dry (conv. to 14% mc)\", \"Rice\", melt_value=\"Farmgate Price\")\n",
        "df4.dropna(inplace=True)\n",
        "\n",
        "# Cropyield (Area Harvested) of Rice\n",
        "df = pd.read_csv(\"datasets/cropyield/cropyield-palay-corn_quarterly_2010-2023.csv\", skiprows=1)\n",
        "df = preprocess_baesians_1(df, \"Palay\", \"Rice\", melt_value=\"Area Harvested\")\n",
        "df = df[df[\"Period\"].str.contains(\"Q\")]\n",
        "\n",
        "# Consumer Price Index (All Income) per Region of Rice\n",
        "df6 = pd.read_csv(\"datasets/price-indices-2018-based/cpi_all-income-households-by-cg-with-backcasting_1994-2023.csv\")\n",
        "df6 = get_quarter_columns(df6, range(2012, 2024), 2)\n",
        "df6 = preprocess_baesians_1(df6, \"01.1.1.12 - Rice\", \"Rice\", melt_value=\"CPI All Income\")\n",
        "\n",
        "# Consumer Price Index (Bottom 30) per Region of Rice\n",
        "df7 = pd.read_csv(\"datasets/price-indices-2018-based/cpi_bottom-30-by-cg-with-backcasting_2012-2017.csv\")\n",
        "df7 = preprocess_baesians_1(df7, \"01.1.1.12 - Rice\", \"Rice\", melt_value=None)\n",
        "tdf = pd.read_csv(\"datasets/price-indices-2018-based/cpi_bottom-30-by-cg_2018-2023.csv\")\n",
        "tdf = preprocess_baesians_1(tdf, \"01.1.1.12 - Rice\", \"Rice\", melt_value=None)\n",
        "df7 = pd.merge(df7, tdf, on=[\"Geolocation\", \"Commodity\"])\n",
        "df7 = get_quarter_columns(df7, range(2012, 2024), 2)\n",
        "df7 = preprocess_baesians_1(df7, \"Rice\", \"Rice\", melt_value=\"CPI Bottom 30\")\n",
        "\n",
        "# Value of Production of Rice (Annual)\n",
        "df8 = df_val_of_prod\n",
        "col = df8.pop(\"Geolocation\")\n",
        "df8.insert(0, col.name, col)\n",
        "df8 = preprocess_baesians_1(df8, \"Rice\", \"Rice\", melt_value=\"Value of Production\")\n",
        "\n",
        "# Stocks of Rice (Annual)\n",
        "df9 = df_stocks\n",
        "\n",
        "# Merge all dfs into a single df NOTE: geolocation must be in lower case\n",
        "# NOTE: barmm is removed because it has null values, unfortunately\n",
        "dfs = [df1, df2, df3, df4, df, df6, df7]\n",
        "df = functools.reduce(lambda left, right: pd.merge(left, right), dfs)\n",
        "\n",
        "# Adding Value of Production (Annual to Quarterly)\n",
        "tdf = df.iloc[:,:3]\n",
        "tdf[\"Period\"] = df[\"Period\"].str.replace(\" Q\\d\", \"\", regex=True)\n",
        "df[\"Value of Production\"] = pd.merge(tdf, df8)[\"Value of Production\"]\n",
        "\n",
        "# HOTFIX: Adding Stocks (Annual to Quarterly)\n",
        "tdf = df[\"Period\"].str.replace(\" Q\\d\", \"\", regex=True)\n",
        "distributed = []\n",
        "for period in tdf:\n",
        "    distributed.append(df9.loc[int(period), \"Stocks\"])\n",
        "df.insert(3, \"Stocks\", pd.Series(distributed))\n",
        "df = swap_columns(df, \"Retail Price\", \"Area Harvested\")\n",
        "df = swap_columns(df, \"Farmgate Price\", \"Wholesale Price\")\n",
        "\n",
        "for x in [\"Volume\", \"Area Harvested\", \"Stocks\"]:\n",
        "    for y in [\"Retail Price\", \"Wholesale Price\", \"Farmgate Price\", \"CPI All Income\", \"CPI Bottom 30\", \"Value of Production\"]:\n",
        "        fig = px.scatter(df, x=x, y=y, color=\"Geolocation\")\n",
        "        # fig.show()\n",
        "\n",
        "# Correlation Matrix\n",
        "contingency = df.iloc[:, 3:].corr(method=\"spearman\")\n",
        "cont_1 = contingency.iloc[3:, 0:3]\n",
        "fig = px.imshow(cont_1, text_auto=True)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RICE: Volume and Retail Price (Annual-Provincial)\n",
        "# Volume of Rice\n",
        "df1 = read_csv_to_df(\"datasets/agricultural-indicators/Volume_Rice and Corn_quarterly.csv\")\n",
        "df1 = df1[df1[\"Geolocation\"].apply(is_region)]\n",
        "df1 = df1[df1[\"Commodity\"] == \"Palay\"]\n",
        "df1[\"Commodity\"] = df1[\"Commodity\"].apply(lambda val: \"Rice\")\n",
        "df1 = df1.melt(id_vars=[\"Commodity\", \"Geolocation\"], value_vars=df1.columns[2:], var_name=\"Period\", value_name=\"Volume\")\n",
        "df1 = swap_columns(df1, \"Commodity\", \"Geolocation\")\n",
        "\n",
        "# Retail Price of Palay\n",
        "df2 = read_csv_to_df(\"datasets/prices/prices_retail_2012-2023.csv\")\n",
        "df2 = df2[df2[\"Geolocation\"].apply(is_region)]\n",
        "df2.dropna(inplace=True)\n",
        "df2 = get_quarter_columns(df2, range(2012, 2024), 2)\n",
        "df2.dropna(inplace=True)\n",
        "df2 = df2[df2[\"Commodity\"] == 'RICE, REGULAR-MILLED, 1 KG']\n",
        "df2[\"Commodity\"] = df2[\"Commodity\"].apply(lambda val: \"Rice\")\n",
        "df2 = df2.melt([\"Geolocation\", \"Commodity\"], df2.columns[2:], \"Period\", \"Price\")\n",
        "\n",
        "# Volume and Price DataFrame\n",
        "df = pd.merge(df1, df2)\n",
        "df = normalize(df, \"Volume\")\n",
        "df = normalize(df, \"Price\")\n",
        "df[\"Geolocation\"] = df[\"Geolocation\"].apply(lambda val: val.lstrip(\".\"))\n",
        "df = df[df[\"Volume\"] != 0]\n",
        "fig = px.scatter(df, x=\"Volume\", y=\"Price\", color=\"Geolocation\")\n",
        "fig.show()\n",
        "df2\n",
        "\n",
        "# # ------------------\n",
        "\n",
        "# RICE: Volume and Retail Price (Quarterly-Regional)\n",
        "df = read_csv_to_df(\"datasets/agricultural-indicators/Volume_Rice and Corn_quarterly.csv\")\n",
        "df = df[df[\"Geolocation\"].apply(is_region)]\n",
        "df = df[df[\"Commodity\"] == \"Palay\"]\n",
        "df[\"Commodity\"] = df[\"Commodity\"].apply(lambda val: \"Rice\")\n",
        "df = df.melt(id_vars=[\"Commodity\", \"Geolocation\"], value_vars=df.columns[2:], var_name=\"Period\", value_name=\"Volume\")\n",
        "df = swap_columns(df, \"Commodity\", \"Geolocation\")\n",
        "\n",
        "# Retail Price of Palay\n",
        "df2 = read_csv_to_df(\"datasets/prices/prices_retail_2012-2023.csv\")\n",
        "df2 = df2[df2[\"Geolocation\"].apply(is_region)]\n",
        "df2.dropna(inplace=True)\n",
        "df2 = get_quarter_columns(df2, range(2012, 2024), 2)\n",
        "df2.dropna(inplace=True)\n",
        "df2 = df2[df2[\"Commodity\"] == 'RICE, REGULAR-MILLED, 1 KG']\n",
        "df2[\"Commodity\"] = df2[\"Commodity\"].apply(lambda val: \"Rice\")\n",
        "df2 = df2.melt([\"Geolocation\", \"Commodity\"], df2.columns[2:], \"Period\", \"Price\")\n",
        "\n",
        "# Volume and Price DataFrame\n",
        "df = pd.merge(df, df2)\n",
        "df[\"Geolocation\"] = df[\"Geolocation\"].apply(lambda val: val.lstrip(\".\"))\n",
        "df = df[df[\"Volume\"] != 0]\n",
        "fig = px.scatter(df, x=\"Volume\", y=\"Price\", color=\"Geolocation\")\n",
        "# fig.show()\n",
        "\n",
        "# ------------------\n",
        "\n",
        "# RICE: Volume and Retail Price (Quarterly-Provincial)\n",
        "df = read_csv_to_df(\"datasets/agricultural-indicators/Volume_Rice and Corn_quarterly.csv\")\n",
        "df = df[df[\"Geolocation\"].apply(is_province)]\n",
        "df = df[df[\"Commodity\"] == \"Palay\"]\n",
        "df[\"Commodity\"] = df[\"Commodity\"].apply(lambda val: \"Rice\")\n",
        "df = df.melt(id_vars=[\"Commodity\", \"Geolocation\"], value_vars=df.columns[2:], var_name=\"Period\", value_name=\"Volume\")\n",
        "df = swap_columns(df, \"Commodity\", \"Geolocation\")\n",
        "\n",
        "# # Retail Price of Palay\n",
        "df2 = read_csv_to_df(\"datasets/prices/prices_retail_2012-2023.csv\")\n",
        "df2 = df2[df2[\"Geolocation\"].apply(is_province)]\n",
        "df2.dropna(inplace=True)\n",
        "df2 = get_quarter_columns(df2, range(2012, 2024), 2)\n",
        "df2.dropna(inplace=True)\n",
        "df2 = df2[df2[\"Commodity\"] == 'RICE, REGULAR-MILLED, 1 KG']\n",
        "df2[\"Commodity\"] = df2[\"Commodity\"].apply(lambda val: \"Rice\")\n",
        "df2 = df2.melt([\"Geolocation\", \"Commodity\"], df2.columns[2:], \"Period\", \"Price\")\n",
        "\n",
        "# Volume and Price DataFrame\n",
        "df = pd.merge(df, df2)\n",
        "df[\"Geolocation\"] = df[\"Geolocation\"].apply(lambda val: val.lstrip(\".\"))\n",
        "df = df[df[\"Volume\"] != 0]\n",
        "fig = px.scatter(df, x=\"Volume\", y=\"Price\", color=\"Geolocation\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Annual\n",
        "df2 = read_csv_to_df(\"datasets/agricultural-indicators/Volume_Rice and Corn_annual.csv\")\n",
        "df2 = df2.melt(id_vars=[\"Geolocation\", \"Commodity\"], value_vars=df2.columns[2:], var_name=\"Period\", value_name=\"Volume\")\n",
        "df_volume = df2[df2[\"Geolocation\"] == \"PHILIPPINES\"]\n",
        "\n",
        "df2 = read_csv_to_df(\"datasets/prices/prices_retail_2012-2023.csv\")\n",
        "df2 = df2[df2[\"Geolocation\"].apply(is_region)]\n",
        "df2.dropna(inplace=True)\n",
        "df2 = get_annual_columns(df2, range(2012, 2024), 2)\n",
        "df2.melt(id_vars=[\"Geolocation\", \"Commodity\"], value_vars=df2.columns[2:], var_name=\"Year\", value_name=\"Volume\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
