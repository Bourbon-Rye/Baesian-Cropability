{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "* It appears that parents of siblings are simple averages of the siblings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "datapath = Path(\"../datasets/prices\")\n",
    "writepath = Path(\"../datasets/prices-preclean\")\n",
    "\n",
    "def fix_month_year_ordering(df):\n",
    "    cols = df.columns.tolist()\n",
    "    date_month_cols = sorted([dt.datetime.strptime(x, \"%Y %b\") for x in cols[2:]])\n",
    "    date_month_cols = [dt.date.strftime(x, \"%Y %b\") for x in date_month_cols]\n",
    "    cols[2:] = date_month_cols\n",
    "    df = df[cols]\n",
    "    return df\n",
    "\n",
    "files = list(datapath.glob(\"*.csv\"))\n",
    "file = files[0]\n",
    "df = pd.read_csv(file, index_col=0)\n",
    "df.drop(list(df.filter(regex=\"Ave|Annual|2024\")), axis=1, inplace=True)  # Remove Ave, Annual, and 2024 Columns\n",
    "df = fix_month_year_ordering(df)\n",
    "print(df.isna().any(axis=1).sum())\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We noticed that there are a lot of \"<month> Annual\" values that are NA \n",
    "# We explore this\n",
    "for year in range(2018, 2025):\n",
    "    # print(df.filter(regex=f\"{year} Annual\"))\n",
    "    year_na = df.filter(regex=f\"{year} Annual\").isnull().any(axis=1).sum()\n",
    "    print(f\"Dataset filtered to {year} has {year_na} rows with NA values\")\n",
    "# df = df[df.columns.drop(list(df.filter(regex='Annual')))]\n",
    "\n",
    "# null_count = df.isna().any(axis=1).sum()\n",
    "# print('Number of rows with null values:', null_count)\n",
    "# # df.dropna(axis=0, inplace=True)\n",
    "# output_df = df[(df.drop([\"Geolocation\", \"Commodity\"], axis=1) != float(0)).any(axis=1)]\n",
    "# output_df.to_csv(\"datasets/clean/dummy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "df_grouped = df.groupby(\"Commodity Description\")\n",
    "geolocs = df_grouped.get_group(\"0 - ALL ITEMS\").Geolocation\n",
    "geolocs = list(geolocs)\n",
    "geolocs_rels = {}\n",
    "natl = \"PHILIPPINES\"\n",
    "region = None\n",
    "while len(geolocs) != 0:\n",
    "    loc = str(geolocs.pop(0))\n",
    "    if (loc == natl):\n",
    "        # Natl case\n",
    "        geolocs_rels.update({natl:{}})\n",
    "    elif (loc.startswith(\"....\")):\n",
    "        # Province or HUC case (discard if HUC)\n",
    "        if \"City\" in loc:\n",
    "            continue\n",
    "        province = loc.strip(\".\")\n",
    "        geolocs_rels[natl][region].append(province)\n",
    "    elif (loc.startswith(\"..\")):\n",
    "        # Region case\n",
    "        region = loc.strip(\".\")\n",
    "        geolocs_rels[natl].update({region:[]})\n",
    "        \n",
    "with open('region_provinces.json', 'w') as fp:\n",
    "    json.dump(geolocs_rels, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenator\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CONCAT = True\n",
    "MERGE = False\n",
    "datapath = Path(\"../datasets/New\")\n",
    "\n",
    "files = []\n",
    "for file in datapath.glob(\"*.csv\"):\n",
    "    files.append(file)\n",
    "\n",
    "print(files)\n",
    "\n",
    "if CONCAT:\n",
    "    df1 = pd.read_csv(files[0], skiprows=2)\n",
    "    for file in files[1:]:\n",
    "        df2 = pd.read_csv(file, skiprows=2)\n",
    "        df1 = pd.concat([df1, df2], axis=0)\n",
    "print(df1.columns)\n",
    "        \n",
    "if MERGE:\n",
    "    df1 = pd.read_csv(files[0])\n",
    "    df2 = pd.read_csv(files[1])\n",
    "    df1 = pd.merge(df1, df2, on=[\"Geolocation\", \"Commodity Description\"], how='inner')\n",
    "\n",
    "# df1[~df1[\"Geolocation\"].str.contains(\"City\")]\n",
    "\n",
    "df1.to_csv(Path(datapath, \"done/test.csv\"), index=False)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Cleanup\n",
    "# I think it's safe to assume that all 0s are from NAs\n",
    "# Assumes that the first row is the header row\n",
    "# (because when initially downloaded from OpenStat, the first two rows are description and newline)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "def fix_month_year_ordering(df):\n",
    "    cols = df.columns.tolist()\n",
    "    if \"Type\" in cols:\n",
    "        cols.remove(\"Type\")\n",
    "    print(cols)\n",
    "    try:\n",
    "        date_month_cols = sorted([dt.datetime.strptime(x, \"%Y %b\") for x in cols[2:]])\n",
    "        date_month_cols = [dt.date.strftime(x, \"%Y %b\") for x in date_month_cols]\n",
    "    except ValueError as v:\n",
    "        if len(v.args) > 0 and v.args[0].startswith('unconverted data remains: '):\n",
    "            date_month_cols = sorted([dt.datetime.strptime(x, \"%Y %B\") for x in cols[2:]])\n",
    "            date_month_cols = [dt.date.strftime(x, \"%Y %B\") for x in date_month_cols]\n",
    "        else:\n",
    "            raise\n",
    "    cols[2:] = date_month_cols\n",
    "    df = df[cols]\n",
    "    return df\n",
    "\n",
    "datapath = Path(\"../datasets/economic-indicators/\")\n",
    "files = list(datapath.glob(\"*.csv\"))\n",
    "\n",
    "print(files)\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, na_values=[0, \"..\"], encoding=\"cp1252\")\n",
    "    # df.drop(list(df.filter(regex=\"Ave|Annual|2024\")), axis=1, inplace=True)  # Remove Ave, Annual, and 2024 Columns\n",
    "    # zero_rows = df.iloc[:,2:].eq(0).all(axis=1)\n",
    "    # df.iloc[zero_rows, 2:] = np.nan\n",
    "    # df = fix_month_year_ordering(df)\n",
    "    df.to_csv(Path(datapath, \"precleaned\", file.name), index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value of Agricultural Production\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "datapath = Path(\"../datasets/new/value\")\n",
    "files = list(datapath.glob(\"*.csv\"))\n",
    "\n",
    "df1 = pd.read_csv(files[0], na_values=[\".\", \"..\"])\n",
    "region = files[0].stem.split(\"_\")[1]\n",
    "df1.insert(1, \"Geolocation\", pd.Series([region for x in range(df1.shape[0])]))\n",
    "for file in files:\n",
    "    df2 = pd.read_csv(file, na_values=[\".\", \"..\"])\n",
    "    region = file.stem.split(\"_\")[1]\n",
    "    df2.insert(1, \"Geolocation\", pd.Series([region for x in range(df2.shape[0])]))\n",
    "    pd.concat([df1, df2])\n",
    "df1\n",
    "\n",
    "df1.to_csv(\"../datasets/value.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baesians",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
